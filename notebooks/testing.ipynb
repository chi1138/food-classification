{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9cba6f-1abf-413e-b775-88b003181159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder \n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "from collections import OrderedDict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9073317-8363-4f5c-aeaa-ceb2ddde88f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if not use_cuda:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "799cc813-544a-4c89-b21f-ab2814ec14bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define transform and import dataset\n",
    "transforms1 = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.CenterCrop((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transforms2 = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.CenterCrop((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    transforms.RandomHorizontalFlip(p=0.5)\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "full_dataset = ImageFolder('../data/raw/Taiwanese-Food-101', transform=transforms1)\n",
    "\n",
    "# Define generator and set split size\n",
    "generator1 = torch.Generator().manual_seed(38)\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "test_size = val_size = (len(full_dataset) - train_size)//2\n",
    "\n",
    "# Split dataset to train, test, and validation\n",
    "train_dataset, test_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size, val_size], generator = generator1)\n",
    "\n",
    "train_dataset.dataset.transform = transforms2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train_dataset = torch.load('../data/processed/Taiwanese-Food-101/train/train_dataset.pt')\n",
    "val_dataset = torch.load(\"../data/processed/Taiwanese-Food-101/val/val_dataset.pt\")\n",
    "test_dataset = torch.load(\"../data/processed/Taiwanese-Food-101/test/test_dataset.pt\")\n",
    "\n",
    "# Define hypterparameters\n",
    "batch_size = 32\n",
    "LR = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "class_names = test_dataset.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d082f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "state_dict = torch.load('../models/densenet161_unfreeze2_sch')\n",
    "\n",
    "model = models.densenet161(weights = \"DEFAULT\")\n",
    "\n",
    "# Freeze the parameters of the pre-trained model to avoid updating them during training\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "\n",
    "for param in list(model.features.denseblock4.parameters()) + list(model.features.norm5.parameters()) + list(model.features.denseblock3.parameters()):\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Replace the classifier with your custom classifier\n",
    "model.classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(2208, 500)),  # Adjusted input size to match DenseNet-161 output\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('fc2', nn.Linear(500, 101)),  # Adjusted output size to match the number of classes (101 in this case)\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n",
    "\n",
    "\n",
    "# Verify model structure and \n",
    "_ = summary(model.cuda(), (3,224,224))\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "191333c6-dda2-434a-8b9b-fdbbfbd91ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(loader, model, criterion, use_cuda, class_names):\n",
    "    test_loss = 0.\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(tqdm(loader, desc='Testing')):\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            _, pred_top1 = output.topk(1, dim=1)\n",
    "            correct_top1 += torch.sum(pred_top1.view(-1) == target).item()\n",
    "            \n",
    "            _, pred_top5 = output.topk(5, dim=1)\n",
    "            correct_top5 += torch.sum(pred_top5 == target.view(-1, 1).expand_as(pred_top5)).item()\n",
    "            \n",
    "            total += target.size(0)\n",
    "            \n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_predictions.extend(pred_top1.view(-1).cpu().numpy())\n",
    "    \n",
    "    test_loss /= total\n",
    "    top1_accuracy = 100. * correct_top1 / total\n",
    "    top5_accuracy = 100. * correct_top5 / total\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.6f}')\n",
    "    print(f'Test Top-1 Accuracy: {top1_accuracy:.2f}% ({correct_top1}/{total})')\n",
    "    print(f'Test Top-5 Accuracy: {top5_accuracy:.2f}% ({correct_top5}/{total})')\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Sample ID': range(1, total + 1),\n",
    "        'Ground Truth': [class_names[t] for t in all_targets],\n",
    "        'Predicted Label': [class_names[p] for p in all_predictions]\n",
    "    })\n",
    "    \n",
    "    return df, test_loss, top1_accuracy, top5_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "575dcc6f-20be-451b-9c80-4c2deaaca260",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6471dc050fdc4d9c9ee25811a61ec947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.307990\n",
      "Test Top-1 Accuracy: 91.98% (2787/3030)\n",
      "Test Top-5 Accuracy: 98.91% (2997/3030)\n"
     ]
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "df, test_loss, top1_accuracy, top5_accuracy = test(test_loader, model, criterion, use_cuda, class_names)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python310",
   "name": "common-cu121.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121:m123"
  },
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
